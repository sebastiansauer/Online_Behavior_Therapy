---
title: "BiPOCD_Modeling_1"
author: "Sebastian Sauer"
date: "21 Juli 2016"
output: 
  html_document:
    code_folding: hide
     toc: yes
     toc_depth: 3
     number_sections: true
---



```{r setup, include=FALSE}

cat("***starting\n***")

knitr::opts_chunk$set(echo = FALSE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)


library(pscl)  # pseudo R^2
library(magrittr) # piping
library(mice)  # imputation
library(testthat)
library(readr) # csv import
library(knitr) # rmarkdown
library(tidyr) # tibbling objects
library(caret) # modeling
library(ggplot2) # plotting
library(tibble) # tibble
library(dplyr) # data wrangling


# results/ variables
mod_results <- list()


# overhead
write_to_file <- FALSE
recompute <- FALSE
test_it <- TRUE
```

# Modeling BiPOCD



```{r read_data}
data_class <- read_csv("../raw_data/data_class.csv")
data_class$responder_3m_f <- factor(data_class$responder_3m_f)
```


Project directory should be "/analysis"; project directory is:
```{r}
getwd()
```



```{r function_write_model_results}

save_model_results <- function(obj,
                               test_df = test_sample,
                               predict_results = TRUE,
                               report_varimp = TRUE){

  results <- list()

  results$fit <- obj
  results$name <- deparse(substitute(obj))


  if (predict_results == TRUE){
    fit_pred <- predict(obj, test_df)
    print(fit_pred)
    results$prediction_results <- fit_pred
    results$confusion_matrix <- confusionMatrix(data = fit_pred, test_df$responder_3m_f)
    print(results$confusion_matrix)
  }

  if (report_varimp == TRUE){
    results$varimp <- varImp(obj, scale = FALSE)
    print(results$varimp)
  }

  return(results)
}

```



# classification (dichotomous)

As we have two outcomes, one dichotomous and one metric, we can both do classification and regression. Let's look at classification first.


Before I forget: we need to exclude the metric outcome variable `CYBOCS_3m`.

```{r exclude_CYBOCS_3M}
if ("CYBOCS_3m" %in% names(data_class)) {
  data_class <- dplyr::select(data_class, -CYBOCS_3m)
}

if ("CYBOCS_3m" %in% data_class) {
  cat("CYBOCS_3m still in data set!, better exclude it\n")
}else {
  cat("CYBOCS_3m has been excluded from the data set\n")}
```

Numer of columns in `data_class`: `r ncol(data_class)`.


```{r data_class_train_test}

set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train_sample <- data_class[trainIndex, ]
test_sample <- data_class[-trainIndex, ]


test_split_data <- function(){
cat("Class of DV:\n")
print(str(data_class$responder_3m_f))
print(str(train_sample$responder_3m_f))
print(str(test_sample$responder_3m_f))


if ("CYBOCS_3m" %in% names(test_sample)) cat("CYBOCS_3m is in test sample")
if (!("CYBOCS_3m" %in% names(train_sample))) cat("CYBOCS_3m is NOT in test sample")
cat("\n")
cat("done\n")
}

if (test_it ==  TRUE) test_split_data()
```



## LM

Let's fit a linear logistic model. The model showed some problems with factor variables that have sparsely populated levels. Additionally, the ratio of variables to observations is not really nice. We probably will not arrive anywhere with this model.




```{r glmfit1, warning = FALSE, message = TRUE}

do_glmfit1 <- function(data_df = data_class){

  ctrl <- trainControl(method = "repeatedcv", 
                       repeats = 5,
                       number = 10)
  
  glmfit1 <- train(factor(responder_3m_f) ~ .,
                   data = data_df,
                   method = "glm",
                   trControl = ctrl)
  
  return(glmfit1)

}

glmfit1 <- do_glmfit1(train_sample)
summary(glmfit1)
print(glmfit1)
confusionMatrix(glmfit1)

mod_results$glmfit1 <- save_model_results(glmfit1)
# 
# dummy <- predict(glmfit1, test_sample)
# 
# confusionMatrix(dummy, test_sample$responder_3m_f)
# str(test_sample$responder_3m_f)

```


Classification rate was ...bad...? But now look at the variable importance. Note: 

> Linear Models: the absolute value of the tâ€“statistic for each model parameter is used. 

(From caret help `varimp`.)


```{r lm1_varimp}
mod_results$glmfit1$varimp[[1]] %>% 
  as_tibble %>% 
  #dplyr::select(everything()) %>% 
  rownames_to_column %>% 
  arrange(desc(Overall))%>% 
  top_n(5) %>% 
  kable
```

Only top-5 shown.

Let's look at McFadden's pseudo R^2. Note that we here too have to fit the model using the train sample, but judge perfomance on the test sample. 

```{r glmfitpR2, warning = TRUE, echo = TRUE, message = TRUE}
glmfit1 <- glm(responder_3m_f ~ ., 
               data = train_sample,
               family = "binomial",
               control = list(maxit = 50))
pR2(glmfit1)
```

Here, the algorithm tells us that we ran into problems; it may well be that we have too many predictors. MacFadden's R^2 is 1; appears too good to be true.



Although not undebated among statisticians, let's try a stepwise regression instead (Gelman detests it, but Hastie/Tibshirani seem to be ok with it)

## Stepwise LM

```{r function_glmfit2}

do_glmfit2 <- function(save_results = FALSE, data_df = train_sample, n_repeats = 10,
                       n_folds = 10){
  
  ctrl <- caret::trainControl(method = "repeatedcv", 
                       repeats = n_repeats,
                       number = n_folds)  # folds
  
  data_df <- dplyr::select(data_df, -ID)
  
  glmfit2 <- caret::train(responder_3m_f ~ .,
                   data = data_df,
                   method = "glmStepAIC",
                   trControl = ctrl)   # takes some minutes! *expensive*
  
  
  if (save_results == TRUE) {
    save(glmfit2, file = "data_objects/glmfit2.Rda")
    cat("writing object to file (in working directory)\n")
  }
  
  return(glmfit2)
}


```


```{r compute_glimfit2, include = FALSE}

glmfit2 <- do_glmfit2(data = train_sample, n_repeats = 1)

names(glmfit2$trainingData)

if (recompute == TRUE) {
  glmfit2 <- do_glmfit2()
  }else{
  load(file = "../data_objects/glmfit2.Rda")
  }

```

```{r glmfit2_results}

glmfit2_pred <- predict(glmfit2, newdata = test_sample)
# length(glmfit2_pred)



mod_results$glmfit2$fit <- glmfit2
(mod_results$glmfit2$confusion_matrix <- confusionMatrix(data = glmfit2_pred, test_sample$responder_3m_f))
mod_results$glmfit2$name <- "glmfit2"
#mod_results$glmfit2$varimp <- varImp(glmfit2, scale = FALSE)



# cat("Class of DV:\n")
# str(data_class$responder_3m_f)
# str(train_sample$responder_3m_f)
# str(test_sample$responder_3m_f)

```


Hm, that's not so enlightning. Let's try a different function, `leap::regsubsets`. Let's do forward selection. Max. nr. of predictors to be included: 10. Note: Here we take the full dataset for model selection. But at least, we use statistics which take the number of predictors into account when choosing the "right" model.

```{r glmfit3}
library(leaps)
glmfit3 <- regsubsets(responder_3m_f ~ . , data = dplyr::select(data_class, -ID),
                      nvmax = 10, 
                      method = "forward")

glmfit3_summary <- summary(glmfit3)
glmfit3_summary
# names(glmfit3_summary)
#str(glmfit3)
#which.max(glmfit3$adjr2)


mod_results$glmfit3 <- save_model_results(glmfit3, 
                                          predict_results = FALSE,
                                          report_varimp = FALSE)


```

So let's look which variables have been included to the models (starting with 1 predictor to a model with 10 predictors):


```{r}
for (i in 1:10) {
  cat(paste("***Model with ",i," predictor(s)***\n", paste = ""))
  print(coef(glmfit3, i))
}
```

Ok, but which model (ie, with what number of predictors) should we choose?

Let's look at *adjusted* $R^2$.

```{r}
plot(glmfit3_summary$adjr2, type = "l")
nr_predictors <- which.max(glmfit3_summary$adjr2)
points(nr_predictors, glmfit3_summary$adjr2[nr_predictors], col = "red", cex = 2, pch = 20)
```

That is, the model suggests `r nr_predictors` predictors to be included.

Let's look at $BIC$.

```{r}
plot(glmfit3_summary$bic, type = "l")
nr_predictors <- which.min(glmfit3_summary$bic)
points(nr_predictors, glmfit3_summary$bic[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.


$BIC$ is more strict in terms of allowing predictors to enter the model. So let's look finally to $C_p$:


```{r}
plot(glmfit3_summary$cp, type = "l")
nr_predictors <- which.min(glmfit3_summary$cp)
points(nr_predictors, glmfit3_summary$cp[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.

Ok, so let's stick to `r nr_predictors` predictors. These are:

```{r}
coef(glmfit3, nr_predictors)[-1]
best_preds <- names(coef(glmfit3, nr_predictors))[-1]  # first is "intercept"
```


# LM with reduced number of predictors
Ok, now again a linear (logistic) model with the reduced number of predictors. Remember that we work with a train and test sample all the time.


```{r glmfit4}
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 10,
                     number = 100)


train_best_preds <- dplyr::select(train_sample, one_of(best_preds), responder_3m_f, -ID)
test_best_preds <- dplyr::select(test_sample, one_of(best_preds), responder_3m_f, -ID)

expect_that(names(train_best_preds), is_identical_to(names(test_best_preds)))


glmfit4 <- train(factor(responder_3m_f) ~ .,
                 data = train_best_preds,
                 method = "glm",
                 trControl = ctrl)
summary(glmfit4)


dimnames(summary(glmfit4)$coefficients)[[1]] -> glmfit4_preds

summary(glmfit4)$coefficients %>% as_tibble %>% 
  mutate(predictors = glmfit4_preds) %>% 
  dplyr::select(predictors, Estimate:`Pr(>|z|)`) -> 
  glmfit4_coefs

kable(glmfit4_coefs)

mod_results$glmfit4 <- save_model_results(glmfit4, test_df = test_best_preds)



```


Accuracy as measured by Cohen's Kappa in **train** sample is

```{r}
mod_results$glmfit4$fit$results$Accuracy
```



However, the accuracy in **test** sample is 
```{r}

mod_results$glmfit4$confusion_matrix

```


Looks not too bad.

Ok, interesting, we see that not many variables are statistically significant (p<.05) now. Precisely, those were statistically significant:

```{r}

glmfit4_coefs %>% rename(p_value = `Pr(>|z|)`) -> glmfit4_coefs  
# easier name for typing

glmfit4_coefs %>% 
  filter(p_value < .05) %T>% kable -> signif_preds


names(signif_preds$predictors)


# signif_preds$predictors %in% names(data_class)


```

Ok, interesting.


Maybe let's have a look at the distribution in the sample:

```{r}
data_class %>%
  select(one_of(signif_preds$predictors), responder_3m_f) %>% 
  na.omit() %>% 
  gather(key = variable, value = value, -responder_3m_f) %>% 
  ggplot(aes(x = responder_3m_f, y = value)) +
  facet_wrap(~variable, scales = "free") + geom_boxplot() + geom_jitter()
```


Looks like as if CDI_S_sum_PRE is the most promising of those variables. As far as a linear model can tell, at least. Time to look at different models!
