---
title: "BiPOCD_Modeling_1: Linear models for classification"
author: "Sebastian Sauer"
date: "21 Juli 2016"
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: true
---



```{r setup, include=FALSE}

cat("***starting\n***")


library(testthat)  # testing
library(leaps)  # regression leaps
library(doMC)  # multiple cores
library(pscl)  # pseudo R^2
library(magrittr) # piping
library(mice)  # imputation
library(testthat)
library(readr) # csv import
library(knitr) # rmarkdown
library(tidyr) # tibbling objects
library(caret) # modeling
library(ggplot2) # plotting
library(tibble) # tibble
library(dplyr) # data wrangling
library(purrr)  # functional programming



knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)

knitr::opts_knit$set(root.dir=normalizePath('../'))





# results/ variables
mod_results <- list()
important_predictors <- list()

# overhead

registerDoMC(cores = 4)

```


Define whether we work.
```{r do_we_work}
write_to_file <- TRUE
recompute <- TRUE
test_it <- TRUE
```


# Modeling BiPOCD

We load the data that has been prepared in the previous step. Dimension of the dataset (r/c):

```{r read_data}
data_class <- read_csv("raw_data/data_mod3.csv")
data_class$responder_3m_f <- factor(data_class$responder_3m_f)
dim(data_class)
```



```{r function_write_model_results}

source("analysis/functions/save_model_results.R")


```



The dimensions of the data frame is `r dim("data_mod3.csv")`.

# Basic classification (dichotomous outcome)

As we have two outcomes, one dichotomous and one metric, we can both do classification and regression. Let's look at classification first.


Before I forget: we need to exclude the metric outcome variable `CYBOCS_3m`.

```{r exclude_metric_outcome}

exclude_metric_outcome <- function(data_df = data_class){
  if ("CYBOCS_3m" %in% names(data_df)) {
    data_df <- dplyr::select(data_df, -CYBOCS_3m)
    message("CYBOCS_3m has been excluded from the data set\n")
  }else {
   message("CYBOCS_3m was not in data frame") 
  }
  return(data_df)
}

data_class <- exclude_metric_outcome(data_class)

```

Number of columns in `data_class`: `r ncol(data_class)`.


```{r test_split_data}
test_split_data <- function(data_df = data_class){
  cat("Class of DV:\n")
  print(str(data_df$responder_3m_f))
  print(str(data_df$responder_3m_f))
  print(str(data_df$responder_3m_f))
  
  
  if ("CYBOCS_3m" %in% names(data_df)) warning("CYBOCS_3m is *in* test sample")
  if (!("CYBOCS_3m" %in% names(data_df))) message("CYBOCS_3m is NOT in test sample")
  message("done")
}

test_split_data(data_class)

```


```{r data_class_train_test}

set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train_sample <- data_class[trainIndex, ]
test_sample <- data_class[-trainIndex, ]




if (test_it ==  TRUE) test_split_data()
```



## Linear Model with all predictors included (probably not identified)

Let's fit a linear logistic model. The model showed some problems with factor variables that have sparsely populated levels. Additionally, the ratio of variables to observations is not really nice. We probably will not arrive anywhere with this model.




```{r glmfit1, warning = FALSE, message = TRUE}

do_glmfit1 <- function(data_df = data_class, n_reps = 5){

  ctrl <- trainControl(method = "repeatedcv", 
                       repeats = n_reps,
                       number = 10)
  
  glmfit1 <- train(factor(responder_3m_f) ~ .,
                   data = data_df,
                   method = "glm",
                   trControl = ctrl)
  
  return(glmfit1)

}


expect_true(is.factor(train_sample$responder_3m_f))

glmfit1 <- do_glmfit1(train_sample)
summary(glmfit1)
print(glmfit1)
confusionMatrix(glmfit1)

# undebug(save_model_results)
mod_results$glmfit1 <- save_model_results(glmfit1)
# 
# dummy <- predict(glmfit1, test_sample)
# 
# confusionMatrix(dummy, test_sample$responder_3m_f)
# str(test_sample$responder_3m_f)

```


Classification rate was ...bad...? But now look at the variable importance. Note: 

> Linear Models: the absolute value of the tâ€“statistic for each model parameter is used. 

(From caret help `varimp`.)


```{r lm1_varimp}
mod_results$glmfit1$varimp[[1]] %>% 
  as_tibble %>% 
  #dplyr::select(everything()) %>% 
  rownames_to_column %>% 
  arrange(desc(Overall))%>% 
  top_n(5) %>% 
  kable
```

Only top-5 shown.

Let's look at McFadden's pseudo R^2. Note that we here too have to fit the model using the train sample, but judge perfomance on the test sample. 

```{r glmfitpR2, warning = TRUE, echo = TRUE, message = TRUE}
glmfit1 <- glm(responder_3m_f ~ ., 
               data = train_sample,
               family = "binomial",
               control = list(maxit = 50))
print(pR2(glmfit1))
```

Here, the algorithm tells us that we ran into problems; it may well be that we have too many predictors. MacFadden's R^2 is 1; appears too good to be true. We should not consider this model any further.



Although not undebated among statisticians, let's try a stepwise regression instead (Gelman detests it, but Hastie/Tibshirani seem to be ok with it)

## Stepwise LM

```{r function_glmfit2, cache = FALSE}

do_glmfit2 <- function(save_results = FALSE, data_df = train_sample, n_repeats = 10,
                       n_folds = 10){
  
  ctrl <- caret::trainControl(method = "repeatedcv", 
                       repeats = n_repeats,
                       number = n_folds)  # folds
  
  if ("ID" %in% names(data_df)) data_df <- dplyr::select(data_df, -ID)
  
  glmfit2 <- caret::train(responder_3m_f ~ .,
                   data = data_df,
                   method = "glmStepAIC",
                   trControl = ctrl, 
                   verbose = TRUE)   # takes some minutes! *expensive*
  
  
  if (save_results == TRUE) {
    save(glmfit2, file = "data_objects/glmfit2.Rda")
    cat("writing object to file (in working directory)\n")
  }
  
  return(glmfit2)
}


```


```{r compute_glmfit2, include = FALSE}

# use this to save computation time:
# glmfit2 <- do_glmfit2(data = train_sample, n_repeats = 1)  
# names(glmfit2$trainingData)
# save(glmfit2, file = "data_objects/glmfit2.Rda")


if (recompute == TRUE) {
  glmfit2 <- do_glmfit2()
  save(glmfit2, file = "data_objects/glmfit2.Rda")
  }else{
  load(file = "data_objects/glmfit2.Rda")
  }

```

```{r glmfit2_results}

glmfit2_pred <- predict(glmfit2, newdata = test_sample)
# length(glmfit2_pred)



mod_results$glmfit2$fit <- glmfit2
(mod_results$glmfit2$confusion_matrix <- confusionMatrix(data = glmfit2_pred, test_sample$responder_3m_f))
mod_results$glmfit2$name <- "glmfit2"
# mod_results$glmfit2$varimp <- varImp(glmfit2, scale = FALSE)




```


Kappa was negative. We should not consider this model further.


## Best subsets regression
That last model is not so enlightning. Let's try a different function, `leaps::regsubsets`. Let's do forward selection. Max. nr. of predictors to be included: 10. Note: Here we take the full dataset for model selection. But at least, we use statistics which take the number of predictors into account when choosing the "right" model.

### Select predictors
First, let's use best subset (brut force) search to identify a subset of relevant predictors.

```{r glmfit3}

glmfit3 <- regsubsets(responder_3m_f ~ . , data = data_class,
                      nvmax = 10, 
                      method = "forward")

glmfit3_summary <- summary(glmfit3)
glmfit3_summary
# names(glmfit3_summary)
#str(glmfit3)
#which.max(glmfit3$adjr2)


mod_results$glmfit3 <- save_model_results(glmfit3, 
                                          predict_results = FALSE,
                                          report_varimp = FALSE)


```

So let's look which variables have been included to the models (starting with 1 predictor to a model with 10 predictors):


```{r}
for (i in 1:10) {
  cat(paste("***Model with ",i," predictor(s)***\n", paste = ""))
  print(coef(glmfit3, i))
}
```

Ok, but which model (ie, with what number of predictors) should we choose?

Let's look at *adjusted* $R^2$.

```{r}
plot(glmfit3_summary$adjr2, type = "l")
nr_predictors <- which.max(glmfit3_summary$adjr2)
points(nr_predictors, glmfit3_summary$adjr2[nr_predictors], col = "red", cex = 2, pch = 20)
```

That is, the model suggests `r nr_predictors` predictors to be included.

Let's look at $BIC$.

```{r}
plot(glmfit3_summary$bic, type = "l")
nr_predictors <- which.min(glmfit3_summary$bic)
points(nr_predictors, glmfit3_summary$bic[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.


$BIC$ is more strict in terms of allowing predictors to enter the model. So let's look finally to $C_p$:


```{r}
plot(glmfit3_summary$cp, type = "l")
nr_predictors <- which.min(glmfit3_summary$cp)
points(nr_predictors, glmfit3_summary$cp[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.

Ok, so let's stick to `r nr_predictors` predictors. These are:

```{r}
coef(glmfit3, nr_predictors)[-1]
best_preds <- names(coef(glmfit3, nr_predictors))[-1]  # first is "intercept"
```


### LM with reduced number of predictors
Ok, now again a linear (logistic) model with the reduced number of predictors. Remember that we work with a train and test sample all the time.


```{r glmfit4}
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 10,
                     number = 100)


train_best_preds <- dplyr::select(train_sample, one_of(best_preds), responder_3m_f)
test_best_preds <- dplyr::select(test_sample, one_of(best_preds), responder_3m_f)

expect_that(names(train_best_preds), is_identical_to(names(test_best_preds)))


glmfit4 <- train(factor(responder_3m_f) ~ .,
                 data = train_best_preds,
                 method = "glm",
                 trControl = ctrl)

summary(glmfit4)


dimnames(summary(glmfit4)$coefficients)[[1]] -> glmfit4_preds

summary(glmfit4)$coefficients %>% as_tibble %>% 
  mutate(predictors = glmfit4_preds) %>% 
  dplyr::select(predictors, Estimate:`Pr(>|z|)`) -> 
  glmfit4_coefs

names(glmfit4_coefs) <- c("predictors", "Estimate", "SE", "z_value", "p_value")


glmfit4_coefs %>% 
  mutate_if(is.numeric, funs(round), digits = 2) %>% 
  kable


mod_results$glmfit4 <- save_model_results(glmfit4, test = test_sample)



```


So, the predictors deemed important where:

```{r}
glmfit4_coefs$predictors

```


Accuracy as measured by Cohen's Kappa in **train** sample is

```{r}
mod_results$glmfit4$fit$results$Accuracy
```



However, the accuracy in **test** sample is 
```{r}

mod_results$glmfit4$confusion_matrix

```



Ok, interesting, we see that some variables are statistically significant (p<.05) now. Precisely, those were statistically significant:

```{r}

# glmfit4_coefs %>% rename(p_value = `Pr(>|z|)`) -> glmfit4_coefs  
# easier name for typing

glmfit4_coefs %>% 
  filter(p_value < .05) -> glm_fit4_signif_preds


kable(glm_fit4_signif_preds)

glm_fit4_signif_preds$predictors


# signif_preds$predictors %in% names(data_class)

important_predictors$bestsubset <- glm_fit4_signif_preds$predictos


```

Ok, interesting.

Let's look at the significant predictors in some more detail.

```{r}
train_sample %>% 
  select(one_of(glm_fit4_signif_preds$predictors), responder_3m_f) %>% 
  gather(key = predictor, value = value, -responder_3m_f) %>% 
  ggplot(aes(x = responder_3m_f, y = value)) + 
  geom_boxplot(alpha = .5) +
  geom_jitter() +
  facet_wrap(~predictor, scales = "free")
```




Maybe let's have a look at the distribution in the sample:

```{r}
if (length(glm_fit4_signif_preds$predictors) > 0) {
  data_class %>%
    select(one_of(glm_fit4_signif_preds$predictors), responder_3m_f) %>% 
    gather(key = variable, value = value, -responder_3m_f) %>% 
    ggplot(aes(x = responder_3m_f, y = value)) +
    facet_wrap(~variable, scales = "free") + geom_boxplot() + geom_jitter()
} else {
  message("There are no statistical significant predictors in current model")
}
```

Time to look at different models!


Don't forget to save results to disk...

```{r}
save(mod_results, file = "mod_results.Rda")
save(important_predictors, file = "important_predictors.Rda")
```

We did not alter the data, so no need to same them to disk.