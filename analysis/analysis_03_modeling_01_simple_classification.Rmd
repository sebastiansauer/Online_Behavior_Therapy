---
title: "BiPOCD_Modeling_1: Linear models for classification"
author: "Sebastian Sauer"
date: "21 Juli 2016"
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: true
---



```{r setup, include=FALSE}

cat("***starting\n***")

knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)



library(doMC)  # multiple cores
library(pscl)  # pseudo R^2
library(magrittr) # piping
library(mice)  # imputation
library(testthat)
library(readr) # csv import
library(knitr) # rmarkdown
library(tidyr) # tibbling objects
library(caret) # modeling
library(ggplot2) # plotting
library(tibble) # tibble
library(dplyr) # data wrangling


# results/ variables
mod_results <- list()


# overhead
write_to_file <- TRUE
recompute <- TRUE
test_it <- TRUE
registerDoMC(cores = 4)

```

# Modeling BiPOCD

We load the data that has been prepared in the previous step. Dimension of the dataset (r/c):

```{r read_data}
data_class <- read_csv("../raw_data/data_mod3.csv")
data_class$responder_3m_f <- factor(data_class$responder_3m_f)
dim(data_class)
```


Project directory should be "/analysis"; project directory is:
```{r}
getwd()
```

```{r fnction_test_results, child = 'functions/save_results.Rmd'}
```



# Basic classification (dichotomous outcome)

As we have two outcomes, one dichotomous and one metric, we can both do classification and regression. Let's look at classification first.


Before I forget: we need to exclude the metric outcome variable `CYBOCS_3m`.

```{r child = "functions/exclude_CYBOCS_3M.Rmd"}
```

Numer of columns in `data_class`: `r ncol(data_class)`.


```{r child  = "functions/test_split_data.Rmd"}
```



```{r data_class_train_test}

set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train_sample <- data_class[trainIndex, ]
test_sample <- data_class[-trainIndex, ]




if (test_it ==  TRUE) test_split_data()
```



## LM

Let's fit a linear logistic model. The model showed some problems with factor variables that have sparsely populated levels. Additionally, the ratio of variables to observations is not really nice. We probably will not arrive anywhere with this model.




```{r glmfit1, warning = FALSE, message = TRUE}

do_glmfit1 <- function(data_df = data_class){

  ctrl <- trainControl(method = "repeatedcv", 
                       repeats = 5,
                       number = 10)
  
  glmfit1 <- train(factor(responder_3m_f) ~ .,
                   data = data_df,
                   method = "glm",
                   trControl = ctrl)
  
  return(glmfit1)

}

glmfit1 <- do_glmfit1(train_sample)
summary(glmfit1)
print(glmfit1)
confusionMatrix(glmfit1)

mod_results$glmfit1 <- save_model_results(glmfit1)
# 
# dummy <- predict(glmfit1, test_sample)
# 
# confusionMatrix(dummy, test_sample$responder_3m_f)
# str(test_sample$responder_3m_f)

```


Classification rate was ...bad...? But now look at the variable importance. Note: 

> Linear Models: the absolute value of the tâ€“statistic for each model parameter is used. 

(From caret help `varimp`.)


```{r lm1_varimp}
mod_results$glmfit1$varimp[[1]] %>% 
  as_tibble %>% 
  #dplyr::select(everything()) %>% 
  rownames_to_column %>% 
  arrange(desc(Overall))%>% 
  top_n(5) %>% 
  kable
```

Only top-5 shown.

Let's look at McFadden's pseudo R^2. Note that we here too have to fit the model using the train sample, but judge perfomance on the test sample. 

```{r glmfitpR2, warning = TRUE, echo = TRUE, message = TRUE}
glmfit1 <- glm(responder_3m_f ~ ., 
               data = train_sample,
               family = "binomial",
               control = list(maxit = 50))
pR2(glmfit1)
```

Here, the algorithm tells us that we ran into problems; it may well be that we have too many predictors. MacFadden's R^2 is 1; appears too good to be true.



Although not undebated among statisticians, let's try a stepwise regression instead (Gelman detests it, but Hastie/Tibshirani seem to be ok with it)

## Stepwise LM

```{r function_glmfit2, cache = FALSE}

do_glmfit2 <- function(save_results = FALSE, data_df = train_sample, n_repeats = 10,
                       n_folds = 10){
  
  ctrl <- caret::trainControl(method = "repeatedcv", 
                       repeats = n_repeats,
                       number = n_folds)  # folds
  
  if ("ID" %in% names(data_df)) data_df <- dplyr::select(data_df, -ID)
  
  glmfit2 <- caret::train(responder_3m_f ~ .,
                   data = data_df,
                   method = "glmStepAIC",
                   trControl = ctrl, 
                   verbose = TRUE)   # takes some minutes! *expensive*
  
  
  if (save_results == TRUE) {
    save(glmfit2, file = "data_objects/glmfit2.Rda")
    cat("writing object to file (in working directory)\n")
  }
  
  return(glmfit2)
}


```


```{r compute_glmfit2, include = FALSE}

# use this to save computation time:
#glmfit2 <- do_glmfit2(data = train_sample, n_repeats = 1)  
# names(glmfit2$trainingData)

if (recompute == TRUE) {
  glmfit2 <- do_glmfit2()
  }else{
  load(file = "../data_objects/glmfit2.Rda")
  }

```

```{r glmfit2_results}

glmfit2_pred <- predict(glmfit2, newdata = test_sample)
# length(glmfit2_pred)



mod_results$glmfit2$fit <- glmfit2
(mod_results$glmfit2$confusion_matrix <- confusionMatrix(data = glmfit2_pred, test_sample$responder_3m_f))
mod_results$glmfit2$name <- "glmfit2"
#mod_results$glmfit2$varimp <- varImp(glmfit2, scale = FALSE)



# cat("Class of DV:\n")
# str(data_class$responder_3m_f)
# str(train_sample$responder_3m_f)
# str(test_sample$responder_3m_f)

```


Hm, that's not so enlightning. Let's try a different function, `leaps::regsubsets`. Let's do forward selection. Max. nr. of predictors to be included: 10. Note: Here we take the full dataset for model selection. But at least, we use statistics which take the number of predictors into account when choosing the "right" model.

```{r glmfit3}
library(leaps)
glmfit3 <- regsubsets(responder_3m_f ~ . , data = data_class,
                      nvmax = 10, 
                      method = "forward")

glmfit3_summary <- summary(glmfit3)
glmfit3_summary
# names(glmfit3_summary)
#str(glmfit3)
#which.max(glmfit3$adjr2)


mod_results$glmfit3 <- save_model_results(glmfit3, 
                                          predict_results = FALSE,
                                          report_varimp = FALSE)


```

So let's look which variables have been included to the models (starting with 1 predictor to a model with 10 predictors):


```{r}
for (i in 1:10) {
  cat(paste("***Model with ",i," predictor(s)***\n", paste = ""))
  print(coef(glmfit3, i))
}
```

Ok, but which model (ie, with what number of predictors) should we choose?

Let's look at *adjusted* $R^2$.

```{r}
plot(glmfit3_summary$adjr2, type = "l")
nr_predictors <- which.max(glmfit3_summary$adjr2)
points(nr_predictors, glmfit3_summary$adjr2[nr_predictors], col = "red", cex = 2, pch = 20)
```

That is, the model suggests `r nr_predictors` predictors to be included.

Let's look at $BIC$.

```{r}
plot(glmfit3_summary$bic, type = "l")
nr_predictors <- which.min(glmfit3_summary$bic)
points(nr_predictors, glmfit3_summary$bic[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.


$BIC$ is more strict in terms of allowing predictors to enter the model. So let's look finally to $C_p$:


```{r}
plot(glmfit3_summary$cp, type = "l")
nr_predictors <- which.min(glmfit3_summary$cp)
points(nr_predictors, glmfit3_summary$cp[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.

Ok, so let's stick to `r nr_predictors` predictors. These are:

```{r}
coef(glmfit3, nr_predictors)[-1]
best_preds <- names(coef(glmfit3, nr_predictors))[-1]  # first is "intercept"
```


# LM with reduced number of predictors
Ok, now again a linear (logistic) model with the reduced number of predictors. Remember that we work with a train and test sample all the time.


```{r glmfit4}
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 10,
                     number = 100)


train_best_preds <- dplyr::select(train_sample, one_of(best_preds), responder_3m_f)
test_best_preds <- dplyr::select(test_sample, one_of(best_preds), responder_3m_f)

expect_that(names(train_best_preds), is_identical_to(names(test_best_preds)))


glmfit4 <- train(factor(responder_3m_f) ~ .,
                 data = train_best_preds,
                 method = "glm",
                 trControl = ctrl)
summary(glmfit4)


dimnames(summary(glmfit4)$coefficients)[[1]] -> glmfit4_preds

summary(glmfit4)$coefficients %>% as_tibble %>% 
  mutate(predictors = glmfit4_preds) %>% 
  dplyr::select(predictors, Estimate:`Pr(>|z|)`) -> 
  glmfit4_coefs

kable(glmfit4_coefs)

mod_results$glmfit4 <- save_model_results(glmfit4, test_df = test_best_preds)



```


Accuracy as measured by Cohen's Kappa in **train** sample is

```{r}
mod_results$glmfit4$fit$results$Accuracy
```



However, the accuracy in **test** sample is 
```{r}

mod_results$glmfit4$confusion_matrix

```



Ok, interesting, we see that not many variables are statistically significant (p<.05) now. Precisely, those were statistically significant:

```{r}

glmfit4_coefs %>% rename(p_value = `Pr(>|z|)`) -> glmfit4_coefs  
# easier name for typing

glmfit4_coefs %>% 
  filter(p_value < .05) %T>% kable -> signif_preds


names(signif_preds$predictors)


# signif_preds$predictors %in% names(data_class)


```

Ok, interesting.


Maybe let's have a look at the distribution in the sample:

```{r}
if (length(signif_preds$predictors) > 0) {
  data_class %>%
    select(one_of(signif_preds$predictors), responder_3m_f) %>% 
    gather(key = variable, value = value, -responder_3m_f) %>% 
    ggplot(aes(x = responder_3m_f, y = value)) +
    facet_wrap(~variable, scales = "free") + geom_boxplot() + geom_jitter()
} else {
  message("There are no statistical significant predictors in current model")
}
```

Time to look at different models!


Don't forget to save results to disk...

```{r}
save(mod_results, file = "mod_results.Rda")
```

We did not alter the data, so no need to same them to disk.