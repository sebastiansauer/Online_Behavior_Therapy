---
title: "modeling2: Flexible models for classification"
author: "Sebastian Sauer"
date: "`r Sys.time()`"
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: true
---


```{r setup, include=FALSE}

message("***starting\n***")

library(knitr)
opts_knit$set(root.dir=normalizePath('../'))

knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```


Load libraries, define overhead variables.

```{r libs}

source("analysis/functions/load_libs.R")


# results/ variables
mod_results <- list()


# overhead
test_it <- FALSE
registerDoMC(cores = 4)

```


Define caching-/write-to-disc variables


```{r play_or_work}
write_to_file <- TRUE
recompute <- TRUE
```




Define paths (data, code, output, etc.).
```{r paths}


source("analysis/functions/paths.R")
```

Read data.
```{r read_data}
data_class <- read_csv("raw_data/data_mod3.csv")

load(file = "analysis/mod_results.Rda")

```


Prepare data (eg., change 0-1 numeric to factor...)
```{r prepare_data}

# helper function: check if vector is binary (ie 0 and 1 values only)
is_binary <- function(var){
  return(all(var %in% c(0,1)))
}


if ("ID" %in% names(data_class)) data_class <- data_class %>% select(-ID)  


# change strange factor levels to well-behaved ones
if ("contact" %in% names(data_class)) data_class$contact <- dplyr::recode_factor(data_class$contact, `Self-referral` = "0", `CAMHS referral` = "1")


data_class$responder_3m_f <- factor(data_class$responder_3m_f, labels = c("negative", "positive"))




data_class %>% 
  mutate_if(is_binary, factor) -> data_class


```

Load function `save_model_results`
``` {r fun_save_model_results}
source("analysis/functions/save_model_results.R")
```


# Upfront work


Before I forget: we need to exclude the metric outcome variable `CYBOCS_3m`.

```{r fun_exclude}

source("analysis/functions/exclude_metric_outcome.R")

```

Now, let's split up the data in a test sample and a training sample.


```{r fun_tst}

source("analysis/functions/test_split_data.R")

```


```{r split_sample}
set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train_sample <- data_class[trainIndex, ]
test_sample <- data_class[-trainIndex, ]

if (test_it ==  TRUE) test_split_data()


predictor_names <- names(train_sample)[names(train_sample) != "responder_3m_f"]
outcome_name <- "responder_3m_f"

```



# Lasso 80/20

A "Lasso" is linear model where model coefficients are penalized in order to shrunk them. That's a way to keep a model simple (few predictors). The lasso is one model with often performs well at the same time keeping the advantages of typical linear model. 

We also compute a cross validation (with default values, ie. 10 folds). Alpha ist set to 1, to prevent ridge regression (see glmnet help for details).

The sample was split in a .8 train sample, and a .2 test sample (hence 80/20).


```{r compute_lasso, echo = TRUE}


# start easy

# data_df <- train_sample


data_mm <- model.matrix(responder_3m_f ~ ., data = data_class)

data_mm <- data_mm[, -1]  #exclude intercept as glmnet demeans the data and reports intercept by default: http://stats.stackexchange.com/questions/99546/2-intercept-cooficients-in-glmnet-output

set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train_mm <- data_mm[trainIndex, ]
test_mm <- data_mm[-trainIndex, ]
train_sample <- data_class[trainIndex, ]
test_sample <- data_class[-trainIndex, ]





lasso_cv <- glmnet::cv.glmnet(x = train_mm, 
                        y = train_sample$responder_3m_f, 
                        family = "binomial",
                        alpha = 1)
```


Now, let's check the results.


```{r lasso_results}
summary(lasso_cv)
print(lasso_cv)
```

For help on the results, use `?cv.glmnet`.

Here, different values for tuning parameter `lambda` are shown (the penalty parameters).

Let's look at how the error rate develops as a function of `lambda`.


```{r plot_lasso_alpha}
plot(lasso_cv)
```

Ok, so some small log alpha (close to zero) seem appropriate.

For ths best model, let's look at the shrinked model parameters:
```{r print_lasso_coef}

mod_results$lasso$coef <- coef(lasso_cv, s = "lambda.min")
mod_results$lasso$coef_signif <- mod_results$lasso$coef@Dimnames[[1]][mod_results$lasso$coef@i]
mod_results$lasso$coef 
```


So, in sum, there were `r length(mod_results$lasso$coef_signif)` "significant" (not irrelevant) parameters.

So, with this alpha-penalty, the model achieved the best prediction: `r lasso_cv$lambda.min`.

Now, let's test the model, ie., look at the prediciont (hold-out/test sample). We check both the predicted class (0 vs. 1), as well as the estimated probability for each class.

```{r predict_lasso}
lasso_pred <- predict(lasso_cv, test_mm, s = "lambda.min", type = "response")
lasso_pred

lasso_pred <- predict(lasso_cv, test_mm, s = "lambda.min", type = "class")
lasso_pred
```

Put together in a confusion matrix:

```{r lasso_confusion_matrix}
lasso_conf <- confusionMatrix(lasso_pred, test_sample$responder_3m_f)
lasso_conf

#debug(save_model_results)
mod_results$lasso <- save_model_results(obj = lasso_cv, 
                                        test_df = test_mm, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = lasso_pred,
                                        conf_matrix = lasso_conf)
```



Unfortunately, the restuls tell us that the model did not find anything. None of the predictors was estimated to play any role.


## Lasso 60/40

Let's compute the Lasso again but increase the test sample. 20% test sample amounts to 10 observations only. One could speculate that is is too few for numeric stability.



```{r compute_lasso2}

set.seed(42)
trainIndex_2 <- createDataPartition(data_class$responder_3m_f, p = .6,
                                  list = FALSE,
                                  times = 1)
# length(trainIndex_2)

train_mm_2 <- data_mm[trainIndex_2, ]
test_mm_2 <- data_mm[-trainIndex_2, ]

train_sample_2 <- data_class[trainIndex_2, ]
test_sample_2 <- data_class[-trainIndex_2, ]



lasso_cv_2 <- glmnet::cv.glmnet(x = train_mm_2, 
                        y = train_sample_2$responder_3m_f, 
                        family = "binomial",
                        alpha = 1)
# str(lasso_cv_2)
```


Now, let's check the results.


```{r lasso_results_2}
summary(lasso_cv_2)
print(lasso_cv_2)
coef(lasso_cv_2, s = "lambda.min")

mod_results$lasso_cv_2$coef <- coef(lasso_cv_2, s = "lambda.min")
mod_results$lasso_cv_2$coef_signif <- mod_results$lasso_cv_2$coef@Dimnames[[1]][mod_results$lasso_cv_2$coef@i]
mod_results$lasso_cv_2$coef 

```


So, in sum, there were `r length(mod_results$lasso_cv_2$coef_signif)` "significant" (not irrelevant) parameters in this 60/40 Lasso model.

Now, let's check predictive accuracy.

```{r predict_lasso_2}
lasso_pred_2 <- predict(lasso_cv_2, test_mm_2, s = "lambda.min", type = "response")
lasso_pred_2

lasso_pred_2 <- predict(lasso_cv_2, test_mm_2, s = "lambda.min", type = "class")
lasso_pred_2
```

Put together in a confusion matrix:

```{r lasso_confusion_matrix_2}
lasso_conf_2 <- confusionMatrix(lasso_pred_2, test_sample_2$responder_3m_f)
lasso_conf_2

#debug(save_model_results)
mod_results$lasso_2 <- save_model_results(obj = lasso_cv_2, 
                                        test_df = test_mm_2, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = lasso_pred_2,
                                        conf_matrix = lasso_conf_2)
```



# Random Forest
Hm, let's look at some completely different model: Random Forests.

We use `mtry` (number of chosen variables) as a tuning parameter. `ntree` was set to 1000 (default is 500).

Assuming 4 cores for computation.


## Random Forest 80/20
Let's split up the sample in 80% training, and 20% testing first, as we did above. Computational somewhat expensive.


```{r compute_random_forest}


grid_rf <- expand.grid(.mtry = c(2, 4, 5, 6, 7, 8, 16))

set.seed(42)
rf1 <- caret::train(responder_3m_f ~ ., 
                    data = train_sample,
                    method = "rf",
                    trControl = trainControl(method = "repeatedcv",
                                             number = 10,
                                             repeats = 10),
                    varImp = TRUE,
                    allowParallel = TRUE,
                    metric = "Kappa",
                    ntree = 1000,
                    tuneGrid = grid_rf)

```


Now, let's check the results.

```{r print_rf1}
print(rf1)
print(rf1$finalModel)

ggplot(rf1)
```


Now let's test the model on the hold-out sample:

```{r rf1_predict}
rf1_predict <- predict(rf1, test_sample, type = "raw")
rf1_predict

rf1_conf_m <- confusionMatrix(rf1_predict, test_sample$responder_3m_f)
rf1_conf_m
```


What about the variable importance:

```{r rf_varimp}
plot(varImp(rf1))
plot(varImp(rf1, scale = FALSE))
```

Note that the varImp values are scaled relatively in the first output; ie., the most important variable has value of 100. In the second output, the mean decrease in predictive accuracy is given, when the variable would be randomly permutated (ie., broken). Note that varImp is based on cross validated training data.

Hm, looks not good. True test values for prediction are: `r test_sample$responder_3m_f`.

However, it was predicted that `r rf1_predict`.

Worse than chance... Can that be? Strange.

```{r save_rf1}

mod_results$rf1 <- save_model_results(obj = rf1, 
                                        test_df = test_sample, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = rf1_predict,
                                        conf_matrix = rf1_conf_m)

```



## Random Forest 60/40


Now the same with a 60-40 sample split up.

```{r compute_random_forest_2}


set.seed(42)
rf2 <- caret::train(responder_3m_f ~ ., 
                    data = train_sample_2,
                    method = "rf",
                    trControl = trainControl(method = "repeatedcv",
                                             number = 10,
                                             repeats = 10),
                    varImp = TRUE,
                    allowParallel = TRUE,
                    metric = "Kappa",
                    ntree = 1000,
                    tuneGrid = grid_rf)

```


Now, let's check the results.

```{r print_rf2}
print(rf2)
print(rf2$finalModel)

ggplot(rf2)
```


Now let's test the model on the hold-out sample:

```{r rf2_predict}
rf2_predict <- predict(rf2, test_sample_2, type = "raw")
rf2_predict

rf2_conf_m <- confusionMatrix(rf2_predict, test_sample_2$responder_3m_f)
rf2_conf_m
```


Unfortunately, not convincing at all.


What about the variable importance:

```{r rf_varimp_2}
plot(varImp(rf2))
plot(varImp(rf2, scale = FALSE))

```




```{r save_rf2}

mod_results$rf2 <- save_model_results(obj = rf2, 
                                        test_df = test_sample_2, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = rf2_predict,
                                        conf_matrix = rf2_conf_m)

```

# Support Vector Machines (SVM)
SVM are another algorithm, quite different to the previous ones. Let's see what happens.

We use a radial kernel and rely on defaults for the rest.

Again, 10/10 repeated cv. Accuracy metric is again Kappa.


## SVM Radial 80/20


```{r compute_svm1}

outcome_train <- dplyr::recode(train_sample$responder_3m_f, `0` = "no", `1` = "yes")
outcome_test <- dplyr::recode(test_sample$responder_3m_f, `0` = "no", `1` = "yes")



mycontrol <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 10,
                          classProbs = TRUE)

set.seed(42)
svm1 <- caret::train(x = train_mm,
                     y = outcome_train,
                     method = "svmRadial",
                     trControl = mycontrol,
                     varImp = TRUE,
                     allowParallel = TRUE,
                     tuneLength = 9,
                     metric = "Kappa",
                     allowParallel = TRUE,
                     preProcess = c("center", "scale")
                   )

```


Resulting in:

```{r svm1_results}
print(svm1)
print(svm1$finalModel)
plot(svm1)
```

Saving, training error was zero...

Predicting test sample:

```{r svm1_predict}


svm1_predict <- predict(svm1, test_mm)
svm1_predict
```

And truth is `r test_sample$responder_3m_f`.

Confusion matrix:

```{r svm1_conf_matrix}
svm1_conf_m <- confusionMatrix(svm1_predict, outcome_test)
svm1_conf_m
```


Not very convincing unfortunately...

Variable Importance: Note that SVM do not have an intrinsic varImp algorithm. Instead, ROC values are used.

```{r svm1_varImp}

svm1_varImp <- caret::varImp(svm1, scale = FALSE)
plot(svm1_varImp)

```





```{r save_svm_1}

mod_results$svm_1 <- save_model_results(obj = svm1, 
                                        test_df = test_sample, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = svm1_predict,
                                        conf_matrix = svm1_conf_m)

```

## SVM Radial, 60-40



```{r compute_svm2}

outcome_train_2 <- dplyr::recode(train_sample_2$responder_3m_f, `0` = "no", `1` = "yes")
outcome_test_2 <- dplyr::recode(test_sample_2$responder_3m_f, `0` = "no", `1` = "yes")



mycontrol <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 10,
                          classProbs = TRUE)

set.seed(42)
svm_2 <- caret::train(x = train_mm_2,
                     y = outcome_train_2,
                     method = "svmRadial",
                     trControl = mycontrol,
                     varImp = TRUE,
                     allowParallel = TRUE,
                     tuneLength = 9,
                     metric = "Kappa",
                     allowParallel = TRUE,
                     preProcess = c("center", "scale")
                   )

```


Resulting in:

```{r svm_2_results}
print(svm_2)
print(svm_2$finalModel)
plot(svm_2)
```

Training error was zero...

Predicting test sample:

```{r svm_2_predict}


svm_2_predict <- predict(svm_2, test_mm_2)
svm_2_predict
```

And truth is `r test_sample$responder_3m_f`.

Confusion matrix:

```{r svm_2_conf_matrix}
svm_2_conf_m <- confusionMatrix(svm_2_predict, outcome_test_2)
svm_2_conf_m
```


Slightly better, but not overwhelming. 


VarImp:



```{r svm_2_varImp}

svm_2_varImp <- caret::varImp(svm_2, scale = FALSE)
plot(svm_2_varImp)
```





```{r save_svm_2}

mod_results$svm_2 <- save_model_results(obj = svm_2, 
                                        test_df = test_sample_2, 
                                        predict_results = FALSE,
                                        report_varimp = FALSE,
                                        fit_pred = svm_2_predict,
                                        conf_matrix = svm_2_conf_m)

```


# Sense check

As the results were not clear-cut, let's do a sense check.

## Compare groups of outcome variable for each numeric predictor

If the outcome variables are expected to exert an influence on the binary outcome, they should separate the groups, right?

Ok, let's see.

```{r}

data_class %>% 
  select_if(is.numeric) %>% names -> num_preds

data_class %>% 
  dplyr::select(one_of(num_preds)) %>% 
  na.omit() %>% 
  map(~t.test(. ~ data_class$responder_3m_f)$p.value) %>% 
  as.data.frame %>% 
  gather %>% 
  mutate(pvalue = ifelse(value < .05, "< .05", "ns")) %>% 
  ggplot(aes(x = reorder(key, value), y = value)) + 
  geom_point(aes(color = pvalue, shape = pvalue)) + 
  coord_flip() +
  geom_hline(yintercept = .05, color = "grey", linetype = "dashed") +
  ylab("p value") +
  xlab("predictor") #+
  # ggtitle("Differences (t-test p-value) between the two outcome groups")
  
```

The p-values of the  predictors was:

```{r}
data_class %>% 
  dplyr::select(one_of(num_preds)) %>% 
  na.omit() %>% 
  map(~t.test(. ~ data_class$responder_3m_f)$p.value) %>% 
  as.data.frame %>% 
  gather %>% 
  rename(predictor = key, p_value = value) %>% 
  mutate(p_value = round(.$p_value, 2)) %>% 
  arrange(p_value) -> signif_preds

signif_preds %>% 
  kable
```

The number of predictors with p-value <.05 is `r nrow(signif_preds)`.


So out of all predictors, it appears that only few are statistically significant. That is, the two outcome groups differ statistically significantly on these values:

```{r signif_preds}
data_class %>% 
  dplyr::select(one_of(num_preds)) %>% 
  na.omit() %>% 
  map(~t.test(. ~ data_class$responder_3m_f)$p.value) %>% 
  as.data.frame %>% 
  gather %>% 
  rename(predictor = key, p_value = value) %>% 
  filter(p_value < .05) -> signif_preds

signif_preds %>% 
  filter(p_value < .05) %>% 
  mutate(p_value = round(.$p_value, 4)) %>% 
  arrange(p_value) %>% 
  kable
```


Note that no control for multiple testing was undertaken. Interpret with caution.

Maybe better to look for the effect size in each case:

```{r es_plot_1}
data_class %>% 
  select_if(is.numeric) %>% names -> num_preds


data_class %>% 
  dplyr::select(one_of(num_preds)) %>% 
  # na.omit() %>% 
  map(~t.test(. ~ data_class$responder_3m_f)) %>% 
  map(~compute.es::tes(.$statistic,
                       n.1 = nrow(dplyr::filter(data_class, responder_3m_f == "negative")),
                       n.2 = nrow(dplyr::filter(data_class, responder_3m_f == "positive")))) %>% 
  map(~do.call(rbind, .)) %>% 
  as.data.frame %>% 
  t %>% 
  data.frame %>% 
  rownames_to_column %>% 
  rename(predictor = rowname) -> 
  data_class_effsize



data_class_effsize %>% 
  dplyr::select(predictor, d, l.d, u.d) %>% 
  mutate(sign = ifelse(d > 0, "+", "-")) -> data_class_effsize_short

data_class_effsize_short %>% 
  ggplot(aes(x = reorder(predictor, d), y = d, color = sign)) + 
  geom_hline(yintercept = 0, alpha = .4) +
  geom_point(aes(shape = sign)) + 
  geom_errorbar(aes(ymin = l.d, ymax = u.d)) +
  coord_flip() +
  ylab("effect size (d) with 95% CI") +
  xlab("predictor") +
  ggtitle("A") -> es_plot_1

es_plot_1
  
```



A bit more specifically, let's look at one predictor variable how the groups differ in that variable with regard to effect size (d).

```{r es_plot_example}
data_class %>% 
  dplyr::select(CGI_S_pre, responder_3m_f) %>% 
  ggplot(aes(x = responder_3m_f, y = CGI_S_pre)) +
  geom_boxplot() + 
  geom_jitter() 
  
```

Hm, does not appear to show a strong difference between the groups.


Much more tangible than d is CLES, that's the probability than some one of group 1 will have an higher value than a randomly chosen person from group 2. Let's plot that, but note that it's a function of d (so no new information).

```{r es_plot_2}


data_class_effsize %>% 
  dplyr::select(predictor, cl.d) %>% 
  mutate(sign = ifelse(cl.d > 50, "+", "-")) %>% 
  ggplot(aes(x = reorder(predictor, cl.d), y = cl.d, color = sign)) + 
  geom_hline(yintercept = 50, alpha = .4) +
  geom_point(aes(shape = sign)) + 
  coord_flip() +
  ylab("effect size (Common Language Effect Size) with 95% CI") +
  xlab("predictor") + 
  ggtitle("B") -> es_plot_2

es_plot_2


```


Both plots together:

```{r es_plot_grid}
grid.arrange(es_plot_1, es_plot_2)
```


Don't forget to save results to disk...

```{r}
save(mod_results, file = "mod_results.Rda")
```
