---
title: "modeling2: Flexible models for regression (quantitative outcome)"
author: "Sebastian Sauer"
date: "`r Sys.time()`"
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: true
---

# Setup

```{r setup, include=FALSE}

message("***starting\n***")

knitr::opts_chunk$set(echo = FALSE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)


library(corrr)
library(purrr)
library(doMC)
library(pscl)  # pseudo R^2
library(magrittr) # piping
library(mice)  # imputation
library(testthat)  # testing
library(readr) # csv import
library(knitr) # rmarkdown
library(tidyr) # tibbling objects
library(caret) # modeling
library(ggplot2) # plotting
library(tibble) # tibble
library(dplyr) # data wrangling


# results/ variables
mod_results_regression <- list()


# overhead
write_to_file <- FALSE
recompute <- FALSE
test_it <- FALSE
registerDoMC(cores = 4)


```


```{r paths}
# PROJHOME that's the project home folder; same as R Project working dir
# knitr uses a *different* path, ie. where the present Rmd-file resides. 
# That's why I manually equalize both paths.
knitr_path <- file.path(PROJHOME, "analysis")

# paths_file <- file.path(PROJHOME, "analysis/functions", "paths.R")  # for R
# paths_file <- file.path(PROJHOME, "functions", "paths.R")  # for knitr
paths_file <- file.path(knitr_path, "functions", "paths.R")
source(paths_file)
```


```{r read_data}
data_regr <- read_csv("../raw_data/data_mod2.csv")


data_regr <- data_regr %>% dplyr::select(-ID)  # exclude ID



# change characters to factors
data_regr %>% 
  mutate_if(is.character, factor) -> data_regr


# change strange factor levels to well-behaved ones
data_regr$contact <- dplyr::recode_factor(data_regr$contact, `Self-referral` = "0", `CAMHS referral` = "1")


load(file = "mod_results.Rda")

```


``` {r child = 'functions/save_results.Rmd'}
```


# Upfront work

Before I forget: we need to exclude the categorial outcome variable `responder_3m_f`.

```{r}
if ("responder_3m_f" %in% names(data_regr)) {
  dplyr::select(data_regr, - responder_3m_f) -> data_regr
  message("responder_3m_f has been excluded from data set.")
} else message("responder_3m_f was *not* present in data set.")

```



## CYBOCS_3m

Before starting modelling, it is instructive to visualize (again) the outcome variable. This is particularly true if the outcome variable is some kind of "plastic" variable that lacks a direct connection to physical-biological kinds as it is often the case for questionnaire data.

Histogram:

```{r}
ggplot(data_regr, aes(x = CYBOCS_3m)) + geom_histogram(aes(y = ..density..)) + geom_density()
ggplot(data_regr, aes(x = CYBOCS_3m)) + geom_histogram(aes(y = ..density..), bins = 20) + geom_density()
ggplot(data_regr, aes(x = CYBOCS_3m)) + geom_histogram(aes(y = ..density..), bins = 10) + geom_density()
```

Summary statistics:
```{r}
summary(data_regr$CYBOCS_3m)
```


## split in train vs. test sample

```{r}
set.seed(42)
trainIndex <- createDataPartition(data_regr$CYBOCS_3m, p = .8,
                                  list = FALSE,
                                  times = 1)

train_sample <- data_regr[trainIndex, ]
test_sample <- data_regr[-trainIndex, ]



predictor_names <- names(train_sample)[names(train_sample) != "CYBOCS_3m"]
outcome_name <- "CYBOCS_3m"

```


# Models
## Lasso 80/20
```{r lasso_function, echo = FALSE}

do_lasso <- function(data = data_regr, p = .8, save = FALSE){


  data_mm <- model.matrix(CYBOCS_3m ~ ., data = data)
  
  data_mm <- data_mm[, -1]  #exclude intercept as glmnet demeans the data and reports intercept by default: http://stats.stackexchange.com/questions/99546/2-intercept-cooficients-in-glmnet-output
  
  set.seed(42)
  trainIndex <- createDataPartition(data$CYBOCS_3m, p = p,
                                    list = FALSE,
                                    times = 1)
  
  train_mm <- data_mm[trainIndex, ]
  test_mm <- data_mm[-trainIndex, ]
  train_sample <- data_regr[trainIndex, ]
  test_sample <- data_regr[-trainIndex, ]
  
  
  
  lasso_cv <- glmnet::cv.glmnet(x = train_mm, 
                                y = train_sample$CYBOCS_3m, 
                                family = "gaussian",
                                alpha = 1)  # lasso penalty
  
  results_lasso <- list()
  
  results_lasso$model <- lasso_cv
  
  summary(lasso_cv)
  print(lasso_cv)
  plot(lasso_cv)
  
  coef_lasso <- coef(lasso_cv, s = "lambda.min")
  coef_lasso
  
  results_lasso$coefs <- coef_lasso
  
  coef_lasso_num <- as.numeric(coef_lasso)
  index_coefs_not_zero <- which(coef_lasso_num != 0)
  names_not_zero <- coef_lasso_1@Dimnames[[1]][index_coefs_not_zero]
  values_not_zero <- coef_lasso_1_num[coef_lasso_num != 0]
  
  lasso_coefs_not_zero <- tibble(
    name = names_not_zero,
    value = values_not_zero
  )
  
  results_lasso$lasso_coefs_not_zero <- lasso_coefs_not_zero
  
  ggplot(lasso_coefs_not_zero, aes(x = name, y = value)) +
    geom_point() +
    coord_flip() +
    ggtitle("Non-zero regression values for Lasso GLM net 80/20 sample")
  
  lasso_pred <- predict(lasso_cv, test_mm, s = "lambda.min", type = "response")
  lasso_pred <- as.numeric(lasso_pred)
  
  results_lasso$lasso_pred <- lasso_pred

  pred_obs <- tibble(
    pred = as.numeric(lasso_pred),
    obs = as.numeric(test_sample$CYBOCS_3m)
  )
  
  ggplot(pred_obs, aes(x = obs, y = pred)) + 
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    geom_point() +
    xlim(c(0,20)) +
    ylim(c(0,20)) +
    ggtitle("Test sample (real) values vs. predicted values")
  
  cat(paste("R squared is:", round(R2(pred_obs$pred, pred_obs$obs), 2), "\n"))
  
  results_lasso$R2 <- R2(pred_obs$pred, pred_obs$obs)
  
  cat(paste("RMSE is:", round(RMSE(pred_obs$pred, pred_obs$obs), 2), "\n"))
  
  results_lasso$RMSE <- RMSE(pred_obs$pred, pred_obs$obs)
  
  cat(paste("Mean absolute error is:", round(mean(abs(pred_obs$pred - pred_obs$obs)), 2), "\n"))
  
  results_lasso$mean_abs_error <- mean(abs(pred_obs$pred - pred_obs$obs))

  if (save == TRUE) {
    mod_results$lasso_reg_1 <- save_model_results(obj = lasso_cv, 
                                                  test_df = test_mm, 
                                                  predict_results = FALSE,
                                                  report_varimp = FALSE,
                                                  fit_pred = lasso_pred,
                                                  conf_matrix = "none")
  }

  
  return(results_lasso)
}

```


```{r lasso1, echo = TRUE}

lasso_regr_1 <- do_lasso()
```



## Lasso 60/40

Let's compute the Lasso again but increase the test sample. 20% test sample amounts to 10 observations only. One could speculate that is is too few for numeric stability.


```{r lasso2, echo = TRUE}
lasso_regr_2 <- do_lasso(p = .6)

```


## Random Forest

```{r prepare_data}

  data_mm <- model.matrix(CYBOCS_3m ~ ., data = data)
  
  data_mm <- data_mm[, -1]  #exclude intercept as glmnet demeans the data and reports intercept by default: http://stats.stackexchange.com/questions/99546/2-intercept-cooficients-in-glmnet-output
  
  set.seed(42)
  trainIndex <- createDataPartition(data$CYBOCS_3m, p = .8,
                                    list = FALSE,
                                    times = 1)
  
  train_mm <- data_mm[trainIndex, ]
  test_mm <- data_mm[-trainIndex, ]
  train_sample <- data_regr[trainIndex, ]
  test_sample <- data_regr[-trainIndex, ]

```


```{r compute_rf_1}

control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")

set.seed(42)
rf_regr_1 <- train(x = train_mm, 
                   y = train_sample$CYBOCS_3m,
                   method = "rf",
                   importance = TRUE,
                   tuneLength=15, 
                   trControl=control,
                   ntrees = 1000)

```

Let's look at the importance of the variables; the results come from the train sample. But note that RF always predict on the hold-out-sample (test sample). However, we used the number of trees for tuning (`mtry`). That's why we better also look at the performance of the test sample (later).

```{r rf1_varimp}
rf1_varimp <- varImp(rf_regr_1, scale=FALSE)
print(rf1_varimp)
plot(rf1_varimp)
```

`ADHD` is the most important. `SCAS`, medication and `CGI` come next. The rest of the variables appear not so important.

Note that the mean decrease in accuracy is depicted, ie., the MSE if the respective variable would be randomly permutated. [Source](http://topepo.github.io/caret/variable-importance.html)


Ok, and now we predict the values of the test sample.

```{r rf1_predicdt}
rf1_predict <- predict(rf_regr_1, newdata = test_mm)
rf1_predict
rf1_RMSE_test <- sqrt(mean((rf1_predict - test_sample$CYBOCS_3m)^2))

```

RMSE amounts to `rf1_RMSE_test`.


## SVM
Let's look at Support Vector Machines (SVM) with radial kernel.


```{r SVM1}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")

set.seed(42)
svm_regr_1 <- train(x = train_mm, 
                   y = train_sample$CYBOCS_3m,
                   method = "svmRadial",
                   importance = TRUE,
                   tuneLength=15, 
                   preProc = c("center", "scale"),
                   trControl=control)
```



```{r svm11_varimp}
svm1_varimp <- varImp(svm_regr_1, scale=FALSE)
print(svm1_varimp)
plot(svm1_varimp)
```

As SVM does not provide a variable importance statistics, here the R^2 statistic is presented (for a linear model with one predictor against the Null model).

As can be seen, this model selects a somewhat different array of predictors as being of central concern. `ADHD` is included among the most important ones, but also `SCAS`, `ChOCI` and `EWSASP`.



Ok, and now we predict the values of the test sample.

```{r svm1_predicdt}
svm1_predict <- predict(svm_regr_1, newdata = test_mm)
svm1_RMSE_test <- sqrt(mean((svm1_predict - test_sample$CYBOCS_3m)^2))

```

RMSE amounts to `svm1_RMSE_test`.


## Boosting (Stochastic Gradient Boosting)


```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)

set.seed(42)
gbm_regr_1 <- train(y = train_sample$CYBOCS_3m,
                    x = train_mm,
                    method = "gbm",
                    # importance = FALSE,
                    tuneLength=10, 
                    preProc = c("center", "scale"),
                    trControl=control
                    # n.trees = 1000
                    )
```

Not sure what the warning message is about.


Let's check some results.

```{r}
gbm_regr_1
plot(gbm_regr_1)
```

It appears that fewer boosting iterations lead to better RMSE here.


Variample importance:

```{r svm11_varimp}
gbm1_varimp <- varImp(gbm_regr_1, scale=FALSE)
print(gbm1_varimp)
plot(gbm1_varimp)
```

Somewhat puzzingly, we again find a somewhat different solution as to the most important variables. `CGI`, `ChOCI`, `distance`, and `CDI` appear to have the most importance. Here, the sum of the boosted iterations (mean decrease in accuracy) are used. Well, in sum, not really telling; better to take it are roughly ordinal...




Ok, and now we predict the values of the test sample.

```{r gbm1_predicdt}
gbm1_predict <- predict(gbm_regr_1, newdata = test_mm)
gbm1_RMSE_test <- sqrt(mean((gbm1_predict - test_sample$CYBOCS_3m)^2))

```

RMSE amounts to `gbm1_RMSE_test`. Slightly better.



# Comparison of model results

```{r compute_model_comparison}

regr_results <- tibble(
  RMSE_models = c(lasso_regr_1$RMSE, lasso_regr_2,$RMSE, rf1_RMSE_test, svm1_RMSE_test,   gbm1_RMSE_test),
  name_models = c("Lasso_1", "Lasso_2", "RF", "SVM", "GBM")
)

```
lasso_regr_1

