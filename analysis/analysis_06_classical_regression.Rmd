---
title: "Classical linear regression"
author: "Sebastian Sauer"
date: "`r Sys.time()`"
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: true
---


# Setup

```{r load_knitr}
library(knitr)
```


```{r knitr_opts}

opts_knit$set(root.dir=normalizePath('../'))


knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      warning = FALSE,
                      message = FALSE)
```


Load packages.


```{r libs}

source("analysis/functions/load_libs.R")
```


Define results variables.

```{r results_var}

# results/ variables
mod_results_regression <- list()
regr_varimp <- list()  # var.imp only
```



Prepare overhead.

```{r overhead}
registerDoMC(cores = 4)
```




Play or work (test it and recompute vs. rely on cached data)

```{r play_or_work, echo = TRUE}
recompute <- TRUE
test_it <- TRUE

write_to_file <- FALSE

```



Define paths.

```{r paths}
source("analysis/functions/paths.R")
```

Load data and data-objects.


```{r read_data}
data <- read_csv("raw_data/data_mod3.csv")  

load(file = "data_objects/mod_results.Rda")
load("data_objects/important_predictors.Rda")
load("data_objects/important_predictors_overview.Rda")

# attr(data, spec) <- ""
# attributes(data)
```


Dimension of data:

Rows: `r dim(data)[1]`.
Cols: `r dim(data)[2]`.



Check if *numeric* outcome variables is present in data frame. 
Check if *binary* outcome variable is present in data frame.

```{r detect_outcome_vars}
str_detect(names(data), "CYBOCS") %>% any %>% expect_true

str_detect(names(data), "responder_3m") %>% any %>% expect_true

```


# Simple non-sense regression models

If the number of predictors (p) equals the number of cases (n), then $R^2$ will approach 1. Hence, model fit *per se* is of limited or no value for deciding on the vlaue of a model.





## Numeric outcome

Outcome: `CYBOCS_3m`.

```{r nonsense_lm1}
data %>% 
 sample_n(45, replace = FALSE) %>% 
  na.omit -> data2



if ("responder_3m_f" %in% names(data)) {
  dplyr::select(data2, -responder_3m_f) -> data2}
  

lm1 <- lm(CYBOCS_3m ~ ., data = data2)  
summary(lm1)
```

$R^2$ of this model is: `r summary(lm1)$r.squared`.

## Binary regression models
Outcome: `responder_3m_f`.

```{r nonsense_lm2}
data %>% 
 sample_n(45, replace = FALSE) %>% 
  na.omit -> data2


if ("CYBOCS_3m" %in% names(data)) {
  dplyr::select(data2, -CYBOCS_3m) -> data2}
  

glm(responder_3m_f ~ ., data = data2, familiy = "binomial") %>% summary
```


$R^2$ of this model is: 

```{r rsquared__nonsense_model}
glm(data2$responder_3m_f ~ .,
               data = data2,
               family = "binomial",
               control = list(maxit = 50)) %>% 
  pR2(.) %>% 
  print

```



# Bivariate variable selection models (regression/ numeric outcome)

Now, let's take the *full* sample, and check whether/how strong each predictor is correlated with the (metric) outcome. Those predictors will be used for the predicting the outcome in the next step.

As the sample size is low, it can be argued that it is better not to split up the sample, but to use some overfitting-aware measure on the performance of the whole sample. Adjusted $R^2$ would be some sensible measure.


```{r univariate_feature_selection_classical_lm1}
data %>% 
  dplyr::select(-c(responder_3m_f, CYBOCS_3m)) %>% 
  map(~lm(data$CYBOCS_3m ~ .x, data = data)) %>% 
  map(summary) %>% 
  map("coefficients") %>% 
  map_dbl(8) %>% # 8th value is p-value
  tidy %>% 
  dplyr::rename(pvalue = x) %>% 
  dplyr::arrange(pvalue)  -> mod_results$classical_01$univariate_lm_selection


# rename(p.value = x)

kable(mod_results$classical_01$univariate_lm_selectio)
```

According to this reasoning, we should retain the following variables (p < .05):


```{r lm1_signif_prdictors}
mod_results$classical_01$univariate_lm_selection %>% 
  filter(pvalue < .05) -> mod_results$classical_01$univariate_lm_selection_signif
  
kable(mod_results$classical_01$univariate_lm_selection_signif)
```

In sum, `r nrow(mod_results$classical_01$univariate_lm_selection_signif)` variables were chosen. 



## Run regression with univariate predictor selection

We take these predictors (which showed a statistical significant assocation with the outcome in bivariate regressions), and submit them to a multivariate regression.

```{r classical_lm1_results}

data %>% 
  dplyr::select(one_of(mod_results$classical_01$univariate_lm_selection_signif$names)) -> data_regr_univar_signif

lm(data$CYBOCS_3m ~ . , data = data_regr_univar_signif) %>% 
  summary %>% 
  tidy %>% 
  dplyr::rename(predictor = term, b = estimate, SE = std.error, T = statistic, p = p.value) %>%   mutate_if(is.numeric, round, digits = 3) %>% 
  arrange(p) ->  mod_results$classical_01$lm1_tidy



mod_results$classical_01$lm1_tidy %>% 
  kable


```

## Number of significant predictors

How many predictors reach statistical significance?

```{r lm1_nr_signif_preds}
mod_results$classical_01$lm1_tidy %>% 
  filter(p < .05) %>% 
  nrow
```



The same table as above, somewhat more beautiful:

```{r lm1_stargazer, results = "asis"}
stargazer(mod_results$classical_01$lm1_tidy, type = "html")

```


Let's have a look at the predictors which reached statistical significance:

```{r lm1_results_signif_only}
mod_results$classical_01$lm1_tidy %>% 
  filter(p < .05) %>% 
  mutate_if(is.numeric, round, digits = 2) -> mod_results$classical_01$signif_preds

mod_results$classical_01$signif_preds %>% 
  kable

```

## Adj. R squared

*Adjusted* $R^2$ of the *whole* dataset is:

```{r lm1_adj_rsquared}
lm(data$CYBOCS_3m ~ . , data = data_regr_univar_signif) %>% 
  summary(.) %>% 
  .$adj.r.squared
```

## Variable importance

Let's take the p-value of the univariate regression (ie., with one predictor) as the variable importance of that predictor.

```{r lm1_varimp}

if (nrow(mod_results$classical_01$signif_preds) > 0 ){
  mod_results$classical_01$signif_preds$rank <-   min_rank(mod_results$classical_01$signif_preds$p)
  
  if (test_it) message("if_clause is entered")
  
  tibble(
    name =  mod_results$classical_01$signif_preds$predictor,
    value = mod_results$classical_01$signif_preds$b,
    rank = min_rank(mod_results$classical_01$signif_preds$p)
  ) -> regr_varimp$classical_lm1
} else {
  regr_varimp$classical_lm1 <- NA
  if (test_it) message("if_clause is NOT entered")
}

```




# Bivariate variable selection models (classification/ binary outcome)

Now, let's take the *full* sample, and check whether/how strong each predictor is associated with the (*binary*) outcome. Those predictors will be used for the predicting the outcome in the next step.

As the sample size is low, it can be argued that it is better not to split up the sample, but to use some overfitting-aware measure on the performance of the whole sample. Adjusted $R^2$ would be some sensible measure.


```{r univariate_feature_selection_glm2}
data %>% 
  dplyr::select(-c(responder_3m_f, CYBOCS_3m)) %>% 
  map(~glm(data$responder_3m_f ~ .x, data = data, family = "binomial")) %>% 
  map(summary) %>% 
  map("coefficients") %>% 
  map_dbl(8) %>%  # that's the p-value
  tidy %>% 
  dplyr::rename(pvalue = x) %>% 
  dplyr::arrange(pvalue)  -> mod_results$classical_02$univariate_lm_selection


# rename(p.value = x)

kable(mod_results$classical_02$univariate_lm_selection)
```

According to this reasoning, we should retain the following variables (p < .05):


```{r classical_glm_02_signif_prdictors}
mod_results$classical_02$univariate_lm_selection %>% 
  filter(pvalue < .05) -> mod_results$classical_02$univariate_lm_selection_signif
  
kable(mod_results$classical_02$univariate_lm_selection_signif)
```

In sum, `r nrow(mod_results$classical_02$univariate_lm_selection_signif)` variables were chosen. 



## Run classification with univariate predictor selection

We take these predictors (which showed a statistical significant assocation with the outcome in bivariate regressions), and submit them to a multivariate regression.

```{r classical_glm2_results}

data %>% 
  dplyr::select(one_of(mod_results$classical_02$univariate_lm_selection_signif$names)) -> data_regr_univar_signif

lm(data$CYBOCS_3m ~ . , data = data_regr_univar_signif) %>% 
  summary %>% 
  tidy %>% 
  dplyr::rename(predictor = term, b = estimate, SE = std.error, T = statistic, p = p.value) %>%   mutate_if(is.numeric, round, digits = 3) %>% 
  arrange(p) ->  mod_results$classical_02$lm1_tidy



mod_results$classical_02$lm1_tidy %>% 
  kable


```

## Number of significant predictors

How many predictors reach statistical significance?

```{r glm2_nr_signif_preds}
mod_results$classical_02$lm1_tidy %>% 
  filter(p < .05) %>% 
  nrow
```



The same table as above, somewhat more beautiful:

```{r glm2_stargazer, results = "asis"}
stargazer(mod_results$classical_02$lm1_tidy, type = "html")

```


Let's have a look at the predictors which reached statistical significance:

```{r glm2_results_signif_only}
mod_results$classical_02$lm1_tidy %>% 
  filter(p < .05) %>% 
  mutate_if(is.numeric, round, digits = 2) -> mod_results$classical_02$signif_preds

mod_results$classical_02$signif_preds %>% 
  kable

```

## Adj. R squared

*Adjusted* $R^2$ of the *whole* dataset is:

```{r adj_rsquared_glm2}

glm(data$responder_3m_f ~ .,
               data = data_regr_univar_signif,
               family = "binomial",
               control = list(maxit = 50)) %>% 
  pR2(.) %>% 
  print

```

## Variable importance

Let's take the p-value of the univariate regression (ie., with one predictor) as the variable importance of that predictor.

```{r glm2_varimp}

if (nrow(mod_results$classical_02$signif_preds) > 0 ){
  mod_results$classical_02$signif_preds$rank <-   min_rank(mod_results$classical_02$signif_preds$p)
  
  if (test_it) message("if_clause is entered")
  
  tibble(
    name =  mod_results$classical_02$signif_preds$predictor,
    value = mod_results$classical_02$signif_preds$b,
    rank = min_rank(mod_results$classical_02$signif_preds$p)
  ) -> regr_varimp$classical_glm2
} else {
  regr_varimp$classical_glm2 <- NA
  if (test_it) message("if_clause is NOT entered")
}

```


# Clean-up

Save results

```{r save_results}
save(mod_results, file = "data_objects/mod_results.Rda")
save(regr_varimp, file = "data_objects/regr_varimp.Rda")

```






