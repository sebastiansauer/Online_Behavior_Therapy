---
title: "BiPOCD_Modeling_2"
author: "Sebastian Sauer"
date: "21 Juli 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE)

library(pscl)  # pseudo R^2
library(magrittr) # piping
library(mice)  # imputation
library(readr) # csv import
library(knitr) # rmarkdown
library(tidyr) # tibbling objects
library(caret) # modeling
library(ggplot2) # plotting
library(tibble) # tibble
library(dplyr) # data wrangling


# results/ variables
mod_results <- list()


# overhead
write_to_file <- FALSE
recompute <- FALSE
```



```{r read_data}
data_class <- read_csv("data_class.csv")
data_class$responder_3m_f <- factor(data_class$responder_3m_f)
getwd()
```




```{r function_write_model_results}

save_model_results <- function(obj,
                               predict_results = TRUE,
                               report_varimp = TRUE){

  results <- list()

  results$fit <- obj
  results$name <- deparse(substitute(obj))


  if (predict_results == TRUE){
    fit_pred <- predict(obj, test)
    results$prediction_results <- fit_pred
    results$confusion_matrix <- confusionMatrix(data = fit_pred, test$responder_3m_f)
  }

  if (report_varimp == TRUE){
    results$varimp <- varImp(obj, scale = FALSE)
  }

  return(results)
}

```



# classification (dichotomous)

As we have two outcomes, one dichotomous and one metric, we can both do classification and regression. Let's look at classification first.


Before I forget: we need to exclude the metric outcome variable `CYBOCS_3m`.

```{r data_class_train_test}

set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .8,
                                  list = FALSE,
                                  times = 1)

train <- data_class[trainIndex, ]
test <- data_class[-trainIndex, ]

cat("Class of DV:\n")
str(data_class$responder_3m_f)
str(train$responder_3m_f)
str(test$responder_3m_f)
```



## LM

Let's fit a linear logistic model. The model showed some problems with factor variables that have sparsely populated levels. Additionally, the ratio of variables to observations is not really nice. We probably will not arrive anywhere with this model.




```{r glmfit1, warning = FALSE, message = TRUE}

do_glmfit1 <- function(){

  ctrl <- trainControl(method = "repeatedcv", 
                       repeats = 10,
                       number = 100)
  
  glmfit1 <- train(factor(responder_3m_f) ~ .,
                   data = train,
                   method = "glm",
                   trControl = ctrl)
  
  return(glmfit1)

}

glmfit1 <- do_glmfit1()

cat("Class of DV:\n")
str(data_class$responder_3m_f)
str(train$responder_3m_f)
str(test$responder_3m_f)


mod_results$glmfit1 <- save_model_results(glmfit1)
# 
# dummy <- predict(glmfit1, test)
# 
# confusionMatrix(dummy, test$responder_3m_f)
# str(test$responder_3m_f)

```


Classification rate was ...bad...? But now look at the variable importance. Note: 

> Linear Models: the absolute value of the tâ€“statistic for each model parameter is used. 

(From caret help `varimp`.)


```{r lm1_varimp}
mod_results$glmfit1$varimp[[1]] %>% 
  as_tibble %>% 
  #dplyr::select(everything()) %>% 
  rownames_to_column %>% 
  arrange(desc(Overall))%>% 
  top_n(5) %>% 
  kable
```

Only top-5 shown.

Let's look at McFadden's pseudo R^2.

```{r glmfitpR2, warning = TRUE, echo = TRUE, message = TRUE}
glmfit1 <- glm(responder_3m_f ~ ., 
               data = train,
               family = "binomial",
               control = list(maxit = 50))
pR2(glmfit1)
```

Here, the algorithm tells us that we ran into problems; it may well be that we have too many predictors. MacFadden's R^2 is 1; appears too good to be true.



Although not undebated among statisticians, let's try a stepwise regression instead.

## Stepwise LM

```{r function_glmfit2}

do_glmfit2 <- function(save_results = FALSE){
  ctrl <- trainControl(method = "repeatedcv", 
                       repeats = 3,
                       number = 50)  # n of iterations for convertion
  
  glmfit2 <- train(responder_3m_f ~ .,
                   data = train,
                   method = "glmStepAIC",
                   trControl = ctrl)   # takes some minutes! *expensive*
  
  
  if (save_results == TRUE) save(glmfit2, file = "glmfit2.Rda")
  
  return(glmfit2)
}


```


```{r glimfit2_results}

if (recompute == TRUE) {
  glmfit2 <- do_glmfit2()
}else{
  load(file = "glmfit2.Rda")
}

glmfit2_pred <- predict(glmfit2, newdata = test)
# length(glmfit2_pred)



mod_results$glmfit2$fit <- glmfit2
(mod_results$glmfit2$confusion_matrix <- confusionMatrix(data = glmfit2_pred, test$responder_3m_f))
mod_results$glmfit2$name <- "glmfit2"
#mod_results$glmfit2$varimp <- varImp(glmfit2, scale = FALSE)



cat("Class of DV:\n")
str(data_class$responder_3m_f)
str(train$responder_3m_f)
str(test$responder_3m_f)

```


Hm, that's not so enlightning. Let's try a different function, `leap::regsubsets`. Let's do forward selection. Max. nr. of predictors to be included: 10. Note: Here we take the full dataset for model selection. But at least, we use statistics which take the number of predictors into account when choosing the "right" model.

```{r glmfit3}
library(leaps)
glmfit3 <- regsubsets(responder_3m_f ~ . , data = data_class,
                      nvmax = 10, 
                      method = "forward")
glmfit3_summary <- summary(glmfit3)
glmfit3_summary
# names(glmfit3_summary)
#str(glmfit3)
#which.max(glmfit3$adjr2)


mod_results$glmfit3 <- save_model_results(glmfit3, 
                                          predict_results = FALSE,
                                          report_varimp = FALSE)


```

So let's look which variables have been included to the models (starting with 1 predictor to a model with 10 predictors):


```{r}
for (i in 1:10) {
  cat(paste("***Model with ",i," predictor(s)***\n", paste = ""))
  print(coef(glmfit3, i))
}
```

Ok, but which model (ie, with what number of predictors) should we choose?

Let's look at *adjusted* $R^2$.

```{r}
plot(glmfit3_summary$adjr2, type = "l")
nr_predictors <- which.max(glmfit3_summary$adjr2)
points(nr_predictors, glmfit3_summary$adjr2[nr_predictors], col = "red", cex = 2, pch = 20)
```

That is, the model suggests `r nr_predictors` predictors to be included.

Let's look at $BIC$.

```{r}
plot(glmfit3_summary$bic, type = "l")
nr_predictors <- which.min(glmfit3_summary$bic)
points(nr_predictors, glmfit3_summary$bic[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.


$BIC$ is more strict in terms of allowing predictors to enter the model. So let's look finally to $C_p$:


```{r}
plot(glmfit3_summary$cp, type = "l")
nr_predictors <- which.min(glmfit3_summary$cp)
points(nr_predictors, glmfit3_summary$cp[nr_predictors], col = "red", cex = 2, pch = 20)
```


That is, the model suggests `r nr_predictors` predictors to be included.

Ok, so let's stick to `r nr_predictors` predictors. These are:

```{r}
coef(glmfit3, nr_predictors)[-1]
best_preds <- names(coef(glmfit3, nr_predictors))[-1]  # first is "intercept"
```


# LM with reduced number of predictors
Ok, now again a linear (logistic) model with the reduced number of predictors. Remember that we work with a train and test sample all the time.


```{r glimfit4}
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 10,
                     number = 100)


train2 <- dplyr::select(train, one_of(best_preds),responder_3m_f)
test2 <- dplyr::select(train, one_of(best_preds), responder_3m_f)


glmfit4 <- train(factor(responder_3m_f) ~ .,
                 data = train2,
                 method = "glm",
                 trControl = ctrl)
summary(glmfit4)


dimnames(summary(glmfit4)$coefficients)[[1]] -> dummy

summary(glmfit4)$coefficients %>% as_tibble %>% 
  mutate(predictors = dummy) %>% 
  dplyr::select(predictors, Estimate:`Pr(>|z|)`) -> 
  dummy

kable(dummy)

mod_results$glmfit4 <- save_model_results(glmfit4)



```


Accuracy as measured by Cohen's Kappa in **train** set is

```{r}
mod_results$glmfit4$fit$results$Accuracy
```



However, the accuracy in **test** set is 
```{r}

mod_results$glmfit4$confusion_matrix

```


Not too exciting.

Ok, interesting, we see that not many variables are statistically significant (p<.05) now:

```{r}

names(dummy)[4] <- "p_value"

dummy %>% 
  filter(p_value < .05) %>% kable
```

