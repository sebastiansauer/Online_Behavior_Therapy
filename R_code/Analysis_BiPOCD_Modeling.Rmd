---
title: "BiPOCD_modeling"
author: "Sebastian Sauer"
date: "11 Juli 2016"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE)


library(readr)
library(knitr)
library(readr)
library(tidyr)
library(caret)
library(ggplot2)
library(tibble)
library(dplyr)



# define paths
path_data <- "~/Documents/OneDrive/Forschung/Online_Behavior_Therapy/raw_data"
path_file_data <- "~/Documents/OneDrive/Forschung/Online_Behavior_Therapy/raw_data/BiPOCD_raw_data.csv"
path_code <- "~/Documents/OneDrive/Forschung/Online_Behavior_Therapy/R_code"
path_figs <- "~/Documents/OneDrive/Forschung/Online_Behavior_Therapy/figs"
path_obj <- "~/Documents/OneDrive/Forschung/Online_Behavior_Therapy/data_objects"



# constants
red1 <- "#880011"




# list with modeling results

mod_results <- vector(mode = "list", length = 10)


```


# Variables for modeling

What variables should be included, and which not?

Include:

- sum scores
- demographics
- outcome
- CYBOCS (all)
- CGI_S_pre
- symptoms: Checking, Obsessions, contamination und symmetry

Exclude:

- nzv variables (near zero variance)
- single items, if collapsed in sum score


(Plus ID variable for joins etc.)

```{r data_mod}


data_mod <- prep_data(data_file = path_file_data)


```



# Replace/omit missing values


## Missings in the DV(s)

How many missings do we have?

```{r}
data_mod %>% select_if(function(col) sum(is.na(col)) != 0) %>% names


data_mod %>% 
  select_if(function(col) sum(is.na(col)) != 0) %>% 
  summarise_each(funs(sum(is.na(.)))) %>% 
  kable
```

Oh no! Some NA's in our outcome variable, that's is a sad story. That reduces the sample size. We could impute, but let's forget that for now. Ok, then be hard better now than later. At least the 5 missings occur in both variables for the same cases. Good bye, guys.

```{r}
data_mod %>% 
  filter(!is.na(responder_3m_f), !is.na(CYBOCS_3m)) -> data_mod
```

The dimension of our data set is now `r dim(data_mod)`.

## NAs in predictors

What about the rest of the variables with NAs? They do have quite a number of NAs, and the variables do not appear so important. So let's omit these variables.

```{r}
data_mod %>% 
  dplyr::select(-OCDonset, -yearswithOCD) -> data_mod
```

Now the dimension of our data set is: `r dim(data_mod)`.


Any missings left?

```{r}
data_mod %>% 
  select_if(function(x) any(is.na(x))) %>% 
  summarise_each(funs(sum(is.na(.)))) %>% 
  kable
```



# Separate train and test dataset

Let's split the data set in two parts, for training (80%) and for testing, to avoid bias from overfitting.

```{r}
set.seed(42)
trainIndex <- createDataPartition(data_mod$responder_3m_f, p = .7,
                                  list = FALSE,
                                  times = 1)

train <- data_mod[trainIndex, ]
test <- data_mod[-trainIndex, ]
```



# Factor variables with sparsely populated levels -- exclude

Some variables are sparsely populated, e.g.:

```{r}
 data_mod %>% 
  dplyr::select(medication) %>% 
  group_by(medication) %>% 
  summarise(n = n()) %>%
  kable
```


Some probems occured with the sparse levels. Better exclude it.

```{r}

sparse_vars <- c("medication", "Education_parent", "OCD_treatm_exp", "Birthcountry", "treatm_exp")

data_mod %>% 
  dplyr::select(-one_of(sparse_vars)) -> data_mod

setwd(path_data)
write_csv(data_mod, path = "data_mod.csv")
```

The factor variables with sparse levels (now excluded) are: `r sparse_vars`.




# classification (dichotomous)

As we have two outcomes, one dichotomous and one metric, we can both do classification and regression. Let's look at classification first.


Before I forget: we need to exclude the metric outcome variable.

```{r}
data_mod %>% 
  dplyr::select(-CYBOCS_3m) -> data_class  # note the new name!

setwd(path_data)
write_csv(data_class, path = "data_class.csv")


set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .7,
                                  list = FALSE,
                                  times = 1)

train <- data_class[trainIndex, ]
test <- data_class[-trainIndex, ]


```


## LM

Let's fit a linear logistic model. The model showed some problems with factor variables that have sparsely populated levels. Additionally, the ratio of variables to observations is not really nice. We probably will not arrive anywhere with this model.




```{r, warning = TRUE, message = TRUE}

ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 10,
                     number = 100)

glmfit1 <- train(responder_3m_f ~ .,
                 data = train,
                 method = "glm",
                 trControl = ctrl,
                 preProc = c("center", "scale"))


glmfit1_pred <- predict(glmfit1, newdata = test)



mod_results[[1]]$fit <- glmfit1
(mod_results[[1]]$confusionMatrix <- confusionMatrix(data = glmfit1_pred, test$responder_3m_f))
mod_results[[1]]$name <- "glmfit1"
mod_results[[1]]$varimp <- varImp(glmfit1, scale = FALSE)

```


Classification rate was ...bad...? But now look at the variable importance. Note: 

> Linear Models: the absolute value of the tâ€“statistic for each model parameter is used. 

(From caret help `varimp`.)


```{r lm1_varimp}
mod_results[[1]]$varimp[[1]] %>% 
  select(everything()) %>% 
  rownames_to_column %>% 
  arrange(desc(Overall))%>% 
  top_n(5) %>% 
  kable
```

Only top-5 shown.

Let's look at McFadden's pseudo R^2.

```{r, warning = TRUE, echo = TRUE, message = TRUE}
library(pscl)
glmfit1 <- glm(responder_3m_f ~ ., 
               data = train,
               family = "binomial",
               control = list(maxit = 50))
pR2(glmfit1)
```

Here, the algorithm tells us that we ran into problems; it may well be that we have too many predictors. MacFadden's R^2 is 1; appears too good to be true.



Although not undebated among statisticians, let's try a stepwise regression instead.

## Stepwise LM

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 3,
                     number = 50)  # n of interations for convertion

glmfit2 <- train(responder_3m_f ~ .,
                 data = train,
                 method = "glmStepAIC",
                 trControl = ctrl,
                 preProc = c("center", "scale"))


glmfit2_pred <- predict(glmfit2, newdata = test)
length(glmfit1_pred)



mod_results[[2]]$fit <- glmfit2
(mod_results[[2]]$confusionMatrix <- confusionMatrix(data = glmfit2_pred, test$responder_3m_f))
mod_results[[2]]$name <- "glmfit2"
mod_results[[2]]$varimp <- varImp(glmfit2, scale = FALSE)

```

Results are well, modest. 

We should try to reduce the number of features.

# Feature selection

Feature selection is a mixture of art and science. And, domain knowledge plays a vital, pivotal role. 

My best guess to compile a "hard core" of predictors would be the following:

- sum scores of psychometrics, including CYBOC_pre_total
- yearswithOCD
- numberdiagnos
- CGI_S_pre

```{r data_mod2}
data_class %>% 
  dplyr::select(dplyr::contains("sum"),
         numberdiagnos,
         CYBOC_pre_total,
         CGI_S_pre,
         ID
         ) %>% 
  left_join({data %>% select(ID, yearswithOCD)}) -> data_class2


set.seed(42)
trainIndex <- createDataPartition(data_class$responder_3m_f, p = .7,
                                  list = FALSE,
                                  times = 1)

train <- data_class[trainIndex, ]
test <- data_class[-trainIndex, ]

```


